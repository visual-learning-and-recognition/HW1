{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 0: Fashion MNIST classification in Pytorch (10 points)\n",
    "\n",
    "The goal of this task is to get you familiar with [Pytorch](https://pytorch.org/), teach you to debug your models, and give you a general understanding of deep learning and computer vision work-flows.\n",
    "\n",
    "[Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) is a dataset of [Zalando’s](https://jobs.zalando.com/tech/) article images — consisting of 70,000 grayscale images in 10\n",
    "categories. Each example is a 28x28 grayscale image, associated with a label from 10 classes. ‘Fashion-\n",
    "MNIST’ is intended to serve as a direct **drop-in replacement** for the original [MNIST](http://yann.lecun.com/exdb/mnist/) dataset — often\n",
    "used as the “Hello, World” of machine learning programs for computer vision. It shares the same image\n",
    "size and structure of training and testing splits. We will use 60,000 images to train the network and\n",
    "10,000 images to evaluate how accurately the network learned to classify images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation directions can be found on pytorch's webpage\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import our network module from simple_cnn.py\n",
    "from simple_cnn import SimpleCNN              # be sure to modify or you may have to restart kernel!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually you'll parse arguments using `argparse` (or similar library) but we can simply use a stand-in object for ipython notebooks. Furthermore, PyTorch can do computations on NVidia `GPU`s or on normal `CPU`s. You can configure the setting using the `device` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS(object):\n",
    "    # input batch size for training \n",
    "    batch_size = 64\n",
    "    # input batch size for testing\n",
    "    test_batch_size=1000\n",
    "    # number of epochs to train for\n",
    "    epochs = 14\n",
    "    # learning rate\n",
    "    lr = 1.0\n",
    "    # Learning rate step gamma\n",
    "    gamma = 0.7\n",
    "    # how many batches to wait before logging training status\n",
    "    log_every = 100\n",
    "    # how many batches to wait before evaluating model\n",
    "    val_every = 100\n",
    "    # set true if using GPU during training\n",
    "    use_cuda = False\n",
    "\n",
    "args = ARGS()\n",
    "device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some basic testing and training code. The testing code prints out the average test loss and the training code (`main`) plots train/test losses and returns the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    \"\"\"Evaluate model on test dataset.\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "    return test_loss, correct / len(test_loader.dataset)\n",
    "\n",
    "def main():\n",
    "    # 1. load dataset and build dataloader\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=args.batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST('../data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=args.test_batch_size, shuffle=True)\n",
    "\n",
    "    # 2. define the model, and optimizer.\n",
    "    model = SimpleCNN().to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    cnt = 0\n",
    "    train_log = {'iter': [], 'loss': [], 'accuracy': []}\n",
    "    test_log = {'iter': [], 'loss': [], 'accuracy': []}\n",
    "    for epoch in range(args.epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # Get a batch of data\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "            # Calculate the loss\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            # Calculate gradient w.r.t the loss\n",
    "            loss.backward()\n",
    "            # Optimizer takes one step\n",
    "            optimizer.step()\n",
    "            # Log info\n",
    "            if cnt % args.log_every == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, cnt, len(train_loader.dataset),\n",
    "                           100. * batch_idx / len(train_loader), loss.item()))\n",
    "                train_log['iter'].append(cnt)\n",
    "                train_log['loss'].append(loss)\n",
    "                # TODO: calculate your train accuracy!\n",
    "                train_log['accuracy'].append(train_acc)\n",
    "            # Validation iteration\n",
    "            if cnt % args.val_every == 0:\n",
    "                test_loss, test_acc = test(model, device, test_loader)\n",
    "                test_log['iter'].append(cnt)\n",
    "                test_log['loss'].append(test_loss)\n",
    "                test_log['accuracy'].append(test_acc)\n",
    "                model.train()\n",
    "            cnt += 1\n",
    "        scheduler.step()\n",
    "    fig = plt.figure()\n",
    "    plt.plot(train_log['iter'], train_log['loss'], 'r', label='Training')\n",
    "    plt.plot(test_log['iter'], test_log['loss'], 'b', label='Testing')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "    fig = plt.figure()\n",
    "    plt.plot(train_log['iter'], train_log['accuracy'], 'r', label='Training')\n",
    "    plt.plot(test_log['iter'], test_log['accuracy'], 'b', label='Testing')\n",
    "    plt.title('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Bug Fix and Hyper-parameter search. (2pts)\n",
    "\n",
    "Simply running `main` will result in a `RuntimeError`! Check out `simple_cnn.py` and see if you can fix the bug. You may have to restart your ipython kernel for changes to reflect in the notebook. After that's done, be sure to fill in the TODOs in `main`.\n",
    "\n",
    "\n",
    "Once you fix the bugs, you should be able to get a reasonable accuracy within 100 iterations just by tuning some hyper-parameter. Include the train/test plots of your best hyperparamter setting and comment on why you think these settings worked best. (you can complete this task on CPU)\n",
    "\n",
    "**YOUR ANSWER HERE**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.batch_size = 64\n",
      "args.epochs = 14\n",
      "args.gamma = 0.7\n",
      "args.log_every = 100\n",
      "args.lr = 1.0\n",
      "args.test_batch_size = 1000\n",
      "args.val_every = 100\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### FEEL FREE TO MODIFY args VARIABLE HERE OR ABOVE ####\n",
    "# args.gamma = float('inf')\n",
    "\n",
    "\n",
    "# DON'T CHANGE\n",
    "# prints out arguments and runs main\n",
    "for attr in dir(args):\n",
    "    if '__' not in attr and attr !='use_cuda':\n",
    "        print('args.{} = {}'.format(attr, getattr(args, attr)))\n",
    "print('\\n\\n')\n",
    "model = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with parameters.(3pt)\n",
    "How many trainable parameters does the trained model have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_count(model):\n",
    "    raise NotImplementedError\n",
    "print('Model has {} params'.format(param_count(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Linear Networks?!? (5pt)\n",
    "Until this point, there are no non-linearities in the SimpleCNN! (Your TAs were just as surprised as you are at the results.) Your next task is to modify the code to add non-linear activation layers, and train your model in full scale. Make sure to add non-linearities at **every** applicable layer.\n",
    "\n",
    "Compute the loss and accuracy curves on train and test sets after 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.epochs = 5\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where did you add your non-linearities? \n",
    "\n",
    "**YOUR ANSWER HERE**\n",
    "\n",
    "\n",
    "<br><br><br>\n",
    "Provide some insights on why the results was fairly good even without activation layers. (**2** pts)\n",
    "\n",
    "**YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
